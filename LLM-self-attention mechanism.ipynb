{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f220daa4",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs)\n",
    "## Self-attention mechanism\n",
    "The **self-attention mechanism** is the backbone of **transformer**-based LLMs, which enables them to understand and generate human-like text by dynamically focusing on relevant parts of the input. This way it captures relationships and dependencies between words, regardless of their distance from one another. \n",
    "- The self-attention mechanism is a key innovation that has driven the success of modern AI.\n",
    "\n",
    "Here, we implement the formulae to get the output of a single self-attention by feeding it the embedded vectors of input data in row vectors stacked into a matrix called $X$.\n",
    "\n",
    "<br>The code is at : https://github.com/ostad-ai/Large-Language-Models\n",
    "<br>Explanation: https://www.pinterest.com/HamedShahHosseini/Deep-Learning/Large-Language-Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e394ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8501262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data of size: (3, 5)\n",
      "[[1 0 1 0 1]\n",
      " [0 2 0 2 3]\n",
      " [2 1 3 1 0]]\n",
      "\n",
      "Attention Weights of size(3, 3):\n",
      "[[1.20392655e-03 8.34065810e-01 1.64730264e-01]\n",
      " [6.43315607e-08 9.87412162e-01 1.25877732e-02]\n",
      " [2.71767270e-06 8.99538710e-01 1.00458572e-01]]\n",
      "\n",
      "Output of size (3, 4):\n",
      "[[1.86325483 2.40016869 3.94122033 1.66696158]\n",
      " [1.72402463 2.18216352 4.03390909 1.30739099]\n",
      " [1.80482602 2.3086879  3.98192516 1.51505179]]\n"
     ]
    }
   ],
   "source": [
    "# An example\n",
    "# Input sequence: 3 tokens, each with 5-dimensional embeddings\n",
    "# so n=3, and d=5\n",
    "X = np.array([\n",
    "    [1, 0, 1, 0 , 1],  # Token 1\n",
    "    [0, 2, 0, 2 , 3],  # Token 2\n",
    "    [2, 1, 3, 1 , 0]   # Token 3\n",
    "])\n",
    "\n",
    "d=X.shape[1] # Dimension of each token\n",
    "d_k = 4 # Dimension of embeddings (often we choose d_k=d, but in this case we didnot)\n",
    "\n",
    "# Randomly initialize the three weight matrices\n",
    "W_Q = np.random.rand(d, d_k)\n",
    "W_K = np.random.rand(d, d_k)\n",
    "W_V = np.random.rand(d, d_k)\n",
    "\n",
    "# Compute Query, Key, and Value vectors\n",
    "# Three matrices: Query, Key, and Value; each with dimension n*d_k\n",
    "Q = X@W_Q\n",
    "K = X@W_K\n",
    "V = X@W_V\n",
    "\n",
    "# Compute attention scores, a matrix of size n*n\n",
    "scores=Q@K.T\n",
    "\n",
    "# Scale the scores by dimension of embeddings\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "# Apply softmax to get attention weights, a matrix of size n*n\n",
    "# actually we compute each row separately, \n",
    "# so the sum of elements of each row is one:\n",
    "# Thus: attention_weights[i].sum()=1, for i=0,1,..,n-1\n",
    "attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores),\n",
    "                                                   axis=1, keepdims=True)\n",
    "\n",
    "# Compute weighted sum of Value vectors\n",
    "# output is a matrix of size n*d_k\n",
    "output=attention_weights@V\n",
    "print(f'Input Data of size: {X.shape}\\n{X}')\n",
    "print(f'\\nAttention Weights of size{attention_weights.shape}:\\n{attention_weights}')\n",
    "print(f'\\nOutput of size {output.shape}:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bf08d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
