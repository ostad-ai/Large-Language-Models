{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542f82ec",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs)\n",
    "## The layer normalization\n",
    "The layer normalization is used in neural networks, including Transformers and Large Language Models (LLMs), to stabilize and improve the training process. It normalizes the activations of a layer across the feature dimension (as opposed to batch normalization, which normalizes across the batch dimension). This helps in reducing internal covariate shift and accelerates training.\n",
    "1. The vector $\\boldsymbol{x}=[x_0,x_1,...,x_{q-1}]^T$ is first normalized to have zero mean and unit variance:\n",
    "<br>$\\hat x_i = \\frac{x_i-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$\n",
    "<br>where $\\mu$ is the mean and $\\sigma^2$ is the variance computed across the feature dimension. $\\epsilon$ is a small positive constant to prevent the possible division by zero.\n",
    "2. The normalized values are then scaled and shifted by:\n",
    "<br>$y_i=\\gamma_i \\hat x_i +\\beta_i$\n",
    "<br>where $\\gamma_i$ and $\\beta_i$ are learnable parameters.\n",
    "\n",
    "In the following, we implement the layer normalization with Numpy. There is a code below in PyTorch to do the same. If you have PyTorch, you can uncomment that code, and compare the results with those of Numpy.\n",
    "<br>The code is at : https://github.com/ostad-ai/Large-Language-Models\n",
    "<br>Explanation: https://www.pinterest.com/HamedShahHosseini/Deep-Learning/Large-Language-Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ee5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b5f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies layer normalization to the input x.\n",
    "#  x: input array of shape (batch_size, sequence_length, feature_dim),\n",
    "#  gamma: Scale parameter of shape (feature_dim,),\n",
    "#  beta: Shift parameter of shape (feature_dim,),\n",
    "#  eps: Small positive constant for numerical stability.\n",
    "# returns: Normalized output of the same shape as x\n",
    "def layer_norm(x, gamma, beta, eps=1e-5):\n",
    "    # Compute mean and variance along the feature dimension (last axis)\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    variance = np.var(x, axis=-1, keepdims=True)\n",
    "    # Normalize\n",
    "    x_normalized = (x - mean) / np.sqrt(variance + eps)\n",
    "    # Apply scale and shift (feature-specific gamma and beta)\n",
    "    return gamma * x_normalized + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda60de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " [[[0.29987269 5.86769799 7.74583217 3.86259778]\n",
      "  [6.03953923 2.46108897 4.47368177 8.63952785]\n",
      "  [6.7957032  3.15739811 5.07548348 1.48722057]]\n",
      "\n",
      " [[6.79718805 7.27155806 8.03218184 5.25528675]\n",
      "  [1.88276552 6.41546367 8.04032614 8.57829672]\n",
      "  [6.81539055 1.93350526 6.55163237 8.41047763]]]\n",
      "--------------------------------------------------\n",
      "Normalized Output:\n",
      " [[[-1.50222353  0.51608268  1.19689604 -0.21075518]\n",
      "  [ 0.2816691  -1.30294166 -0.41172452  1.43299708]\n",
      "  [ 1.33629451 -0.48683991  0.47430198 -1.32375658]]\n",
      "\n",
      " [[-0.04124779  0.42612173  1.17552065 -1.56039458]\n",
      "  [-1.6509401   0.07074483  0.68792717  0.89226811]\n",
      "  [ 0.36781896 -1.65513153  0.25852312  1.02878946]]]\n",
      "--------------------------------------------------\n",
      "Checking the mean and variance:\n",
      "Mean of output:\n",
      "[[-4.16333634e-17  3.88578059e-16 -2.22044605e-16]\n",
      " [ 6.10622664e-16 -1.66533454e-16 -3.88578059e-16]]\n",
      "Variance of output:\n",
      "[[0.99999869 0.99999804 0.99999749]\n",
      " [0.99999029 0.99999856 0.99999828]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "batch_size, sequence_length, feature_dim=2,3,4\n",
    "x = np.random.uniform(0,10,size=(batch_size,sequence_length,feature_dim))\n",
    "\n",
    "# Feature-specific gamma and beta (learnable parameters)\n",
    "gamma = np.ones(feature_dim) # Scale parameters\n",
    "beta = np.zeros(feature_dim)  # Shift parameters\n",
    "\n",
    "# Apply layer normalization\n",
    "output = layer_norm(x, gamma, beta)\n",
    "\n",
    "print(\"Input:\\n\", x)\n",
    "print(50*'-')\n",
    "print(\"Normalized Output:\\n\", output)\n",
    "#----------------------------------------\n",
    "print(50*'-')\n",
    "print('Checking the mean and variance:')\n",
    "mean=np.mean(output,axis=-1)\n",
    "variance=np.mean((output-mean[...,np.newaxis])**2,axis=-1)\n",
    "print(f'Mean of output:\\n{mean}')\n",
    "print(f'Variance of output:\\n{variance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408299e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can compare the results with those of PyTorch\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# layer_norm_tnn = nn.LayerNorm(feature_dim)\n",
    "# # Activate module\n",
    "# layer_norm_tnn(torch.from_numpy(x.astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e155b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
